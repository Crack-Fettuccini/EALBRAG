# Attention-Enhanced Query System with RAG and HyDE

## Overview

This project implements an advanced query processing system that enhances user interactions through a dynamic attention architecture, incorporating Retrieval-Augmented Generation (RAG), History-enhanced Dynamic Exploration (HyDE), and a sliding window mechanism. The system intelligently manages memory, prioritizing the most relevant information to improve response accuracy and contextual understanding.

## Features

1. **Memory Functions within Attention Architecture**:
   - The attention mechanism in this project is designed to retain and track the most attended-to information throughout the model's processing. It allows the system to remember key aspects of user queries and conversation history, dynamically adjusting the attention weights based on what is most relevant. This enables improved coherence in responses and a deeper understanding of user intent over time.

2. **Retrieval-Augmented Generation (RAG)**:
   - RAG enhances the model's ability to retrieve and incorporate relevant external documents during response generation. This system queries a specified folder for documents, utilizing primary and secondary queries to filter and return the most pertinent content, thereby enriching the responses generated by the model.

3. **History-enhanced Dynamic Exploration (HyDE)**:
   - HyDE facilitates the inference of additional contextual information from conversation history. It analyzes past interactions to identify gaps in knowledge and dynamically generates relevant queries, enriching the conversation and providing more personalized responses based on user preferences and previous queries.

4. **Sliding Window Mechanism**:
   - The sliding window architecture manages the input prompt by dividing it into four parts: 
     1. RAG and HyDE data
     2. Prompted data
     3. Primary and secondary queries
     4. Response
   - This method allows for effective token management, ensuring that all parts of the query are optimally filled and adjusted based on the attention dynamics of the ongoing conversation.

## Technologies and Stack Used

- **Programming Language**: Python 3.8+
- **Machine Learning Framework**: PyTorch
- **Transformers Library**: Hugging Face Transformers for model and tokenizer management
- **Data Storage**: Local file system for document storage used in RAG
- **Visualizations**: Matplotlib (optional for any visual representations)
- **Environment Management**: Anaconda or virtualenv (for package management)

## Getting Started

To set up the project and run the code, follow these steps:

### Prerequisites

- Python 3.8 or higher
- Anaconda or virtualenv (optional but recommended for dependency management)
- CUDA (optional for GPU acceleration)

### Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/Crack-Fettuccini/EALBRAG
   cd EALBRAG
   ```

2. **Create a virtual environment (optional)**:

   ```bash
   conda create -n query_system python=3.8
   conda activate query_system
   ```

   Or, if using virtualenv:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

3. **Install required packages**:

   ```bash
   pip install torch transformers matplotlib
   ```

4. **Prepare your documents**:
   - Create a folder named `documents` in the project root directory.
   - Place any relevant text documents (`.txt` files) that you want to use for RAG inside this folder.

### Running the Code

1. **Run the main script**:

   ```bash
   python main.py
   ```

2. **Interact with the system**:
   - Follow the on-screen instructions to input your queries. The system will process the input using RAG, HyDE, and the sliding window mechanism to provide relevant and enriched responses.

### Example Usage

```python
query = "Who are the smartest people in web development?"
response = model.process_query(query)
print(response)
```

## Contribution

Contributions are welcome! If you have any ideas, enhancements, or bug fixes, feel free to create a pull request.

## Acknowledgments

- [Hugging Face](https://huggingface.co) for providing the Transformers library.
- PyTorch for deep learning capabilities.
- The open-source community for their contributions to machine learning and natural language processing.
